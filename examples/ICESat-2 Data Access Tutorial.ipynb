{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979b7001-d08d-499f-a743-37b7af0b21bb",
   "metadata": {},
   "source": [
    "# Tutorial: Accessing ICESat-2 Data\n",
    "\n",
    "icepyx, sliderule, h5coro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9dff2-0e40-4a8f-b80b-ace6dce71cdb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf706c9-e785-4666-8137-0cd128ec5046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e677291-84d5-4c15-8fef-668a908bcf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839c2bc-2804-4c1c-8cdc-aefe3a2a8ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b9b5805-f2c7-4eba-80fd-f7d7ef69f277",
   "metadata": {},
   "source": [
    "## Part 2: SlideRule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27585f37-dcc5-4b5d-af35-ace35b024b93",
   "metadata": {},
   "source": [
    "SlideRule is a collaborative effort between NASA Goddard Space Flight Center (GSFC) and the University of Washington, funded by the ICESat-2 program. It provides on-demand science data processing service for ICESat-2 and GEDI data that runs on Amazon Web Services (AWS) and responds to REST-like API calls to process and return science results. This _science-data-as-a-service_ model is a new way for researchers to work and analyze data, enabling them to have low-latency access to custom-generated, high-level data products.\n",
    "\n",
    "SlideRule users provide specific parameters at the time of the request to compute products that fit their science needs. SlideRule then uses cloud-optimized versions of computational algorithms and a scalable cluster of EC2 instances to process data efficiently. All data is then returned to the user as a `geopandas` GeoDataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606729fd-23bb-4d67-baf3-80f8335baf6b",
   "metadata": {},
   "source": [
    "![SlideRule Overview](sliderule_overview_medium.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a223239-7a33-41e2-9f45-d5a6c9c7b225",
   "metadata": {},
   "source": [
    "### For more information\n",
    "***Website***: https://slideruleearth.io  \n",
    "***Documentation***: https://slideruleearth.io/web/rtd/  \n",
    "***GitHub***: https://github.com/SlideRuleEarth/sliderule  \n",
    "***Examples***: https://github.com/SlideRuleEarth/sliderule-python  \n",
    "***Contact***: support@mail.slideruleearth.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea57d5-e372-4319-8d1a-07a257d69714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the latest version of the sliderule client, run this cell.\n",
    "# It will install the sliderule Python client into your current conda environment.\n",
    "# You will then need to restart your kernel to have the changes take effect.\n",
    "%pip install --quiet \"sliderule>=4.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd8f48-31dd-4ade-8b2d-82b3813d36d4",
   "metadata": {},
   "source": [
    "### Example 1: Just Get Me Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddf020-db0a-4a1f-a474-2cbb2582815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Import the client\n",
    "from sliderule import sliderule, icesat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f01fbf-3745-47f9-95a1-50b2f650f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Initialize the client\n",
    "sliderule.init(\"slideruleearth.io\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667a61b6-b30a-4c1c-9052-307cea35e248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (3) Define an area of interest\n",
    "region = sliderule.toregion(\"grandmesa.geojson\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169f2e32-60db-4e5b-84a8-39d393480109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Specify the processing parameters\n",
    "parms = {\n",
    "    \"poly\": region[\"poly\"],\n",
    "    \"srt\": icesat2.SRT_LAND,\n",
    "    \"len\": 20.0,\n",
    "    \"res\": 100.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ab0c6d-73b9-4145-af0c-15e9f1dcc36a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (5) Make the processing request\n",
    "gdf = icesat2.atl06p(parms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5736b9fa-dc7a-4c63-a392-e051b4805833",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1a0fe0-4f5c-4808-93ac-92171e2e7268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64243077-1cb1-4975-82dd-6984964f28c3",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceca43b-e0f0-41b1-aab2-5cb38402bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101160a-4792-4481-b3db-352ed51f6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_lon = [e[\"lon\"] for e in region[\"poly\"]]\n",
    "region_lat = [e[\"lat\"] for e in region[\"poly\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f915b-d455-4dec-86e6-2ae72f262ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots()\n",
    "ax.set_title(\"ATL06-SR Points\")\n",
    "ax.set_aspect('equal')\n",
    "gdf.plot(ax=ax, column='h_mean', cmap='inferno', s=0.1)\n",
    "ax.plot(region_lon, region_lat, linewidth=1, color='g');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9795f-5a83-4035-9d2d-0be59e710956",
   "metadata": {},
   "source": [
    "#### Explanation of what happened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1d185c-bc41-4b0f-8b1e-fff6af90cb9f",
   "metadata": {},
   "source": [
    "#### (1) Import the client\n",
    "\n",
    "The SlideRule Python client is broken up into different modules:\n",
    "* `sliderule`: core general functionality\n",
    "* `icesat2`: ICESat-2 on-demand, subsetting, raster sampling products\n",
    "* `gedi`: GEDI subsetting, and raster sampling products\n",
    "* `h5`: direct HDF5 data access\n",
    "* `earthdata`: CMR, CMR-STAC, TNM helper functions _(use `earthaccess` instead)_\n",
    "* `io`: reading and writing results to/from local files\n",
    "* `ipysliderule`: toolbox for building SlideRule interfaces in a Jupyter notebook\n",
    "\n",
    "#### (2) Initialize the client\n",
    "\n",
    "Configure the client settings:\n",
    "* `url`: address of sliderule service (default = \"slideruleearth.io\")\n",
    "* `verbose`: display messages from server (default = False)\n",
    "* `loglevel`: criticality of log messages to display (default = logging.INFO)\n",
    "* `organization`: selection of cluster, used for private clusters (default = \"sliderule\")\n",
    "* `desired_nodes`: number of nodes to run in a private cluster (default = None)\n",
    "* `time_to_live`: how long to deploy a private cluster (default = 60 minutes)\n",
    "* `bypass_dns`: query the provisioning system for IP address and don't use DNS lookup hostname (default = False)\n",
    "* `plugins`: check if plugin is present (default = [])\n",
    "* `trust_env`: use netrc file for authentication (default = False)\n",
    "* `log_handler`: attach handler to client logging (default = None)\n",
    "* `rethrow`: immediately rethrow any caught exception inside of the client (default = None)\n",
    "\n",
    "#### (3) Define an area of interest\n",
    "\n",
    "SlideRule uses an area of interest for determining which dataset resources to process and to then subset those resources to provide data only inside the area of interest.  The `sliderule.toregion` function converts multiple input types into a format understood by SlideRule.  The inputs types supported are: geojson, shapefile, GeoDataFrame, list of coordinates, and a dictionary of coordinates.\n",
    "\n",
    "The resources (e.g. granules) to process can always be supplied in any of the processing APIs. But if they are not supplied (which is typical), then to determine which resources to process, the SlideRule server-side code uses the area of interest to make requests to NASA's Common Metadata Repository (CMR) legacy and STAC interfaces, along with USGS's The National Map interface. The server code automatically determines which interfaces should be queried and the parameters of the query needed for properly filtering results. \n",
    "\n",
    "In rare cases when the area of interest is very complex (e.g. a bunch of islands, or an extremely high vertice-count polygon), then the user can request the server to rasterize the area of interest and use it as a mask for determining which data to process.  See https://slideruleearth.io/web/rtd/user_guide/SlideRule.html#geojson for more details.\n",
    "\n",
    "#### (4) Specify the processing parameters\n",
    "\n",
    "There is a multitude of processing parameters that are available to each API.  The ones used here are:\n",
    "* `poly`: area of interest\n",
    "* `srt`: surface reference type; if set to -1 (or icesat2.DYNAMIC), then all surface types are used\n",
    "* `len`: length of the extent (or variable-length segment) of along-track photon clouds to use in processing each posting\n",
    "* `res`: the step size between postings\n",
    "\n",
    "See user's guide for additional parameters: https://slideruleearth.io/web/rtd/index.html\n",
    "\n",
    "#### (5) Make the processing request\n",
    "\n",
    "Under-the-hood this makes an HTTP request to the SlideRule service running in AWS to perform the ATL06 surface-finding algorithm on ATL03 photons to produce an elevation, and then collects the results into a pandas GeoDataFrame.\n",
    "\n",
    "The different ICESat-2 APIs available are:\n",
    "* `atl03sp`: subset and filter ATL03 photons; provide custom YAPC and ATL08 classifications\n",
    "* `atl03v`: fast segment level subsetting of ATL03 photons\n",
    "* `atl06s`: subset the ATL06 land elevation product\n",
    "* `atl06p`: dynamically generate ATL06 surface elevation product\n",
    "* `atl08p`: dynamically generate the ATL08 vegetation density product (PhoREAL)\n",
    "* `atl13p`: subset the ATL13 coastal water product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dee440b-4ae4-4c04-b77e-95c95a3172ef",
   "metadata": {},
   "source": [
    "### Example 2: Sample GEDI Elevation Product at ICESat-2 Dynamically Generated Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb347c9-83e8-40bf-86d5-04415f1a0ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sliderule import sliderule, icesat2, gedi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6552b0-64cc-436e-9b62-02c1923f5cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliderule.init(\"slideruleearth.io\", verbose=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c5d983-7706-4321-ac2e-1d603d33e09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parms = {\n",
    "    \"poly\": sliderule.toregion('grandmesa.geojson')['poly'],\n",
    "    \"t0\": '2019-11-14T00:00:00Z',\n",
    "    \"t1\": '2019-11-15T00:00:00Z',\n",
    "    \"srt\": icesat2.SRT_LAND,\n",
    "    \"len\": 100,\n",
    "    \"res\": 100,\n",
    "    \"pass_invalid\": False, \n",
    "    \"atl08_class\": [\"atl08_ground\", \"atl08_canopy\", \"atl08_top_of_canopy\"],\n",
    "    \"atl08_fields\": [\"h_dif_ref\"],\n",
    "    \"phoreal\": {\"binsize\": 1.0, \"geoloc\": \"center\", \"use_abs_h\": False, \"send_waveform\": False},\n",
    "    \"samples\": {\"gedi\": {\"asset\": \"gedil3-elevation\"}}\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557a700-56d3-43eb-8a2d-0ba876c72bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "atl08 = icesat2.atl08p(parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265f4c5-2d79-4804-aa94-1c5a57d9fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "atl08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00d74c-3516-4c1d-bea9-303adaef30d6",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32a2d4-97ad-4da4-bbf6-a41ed56144f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1b839-d3c1-496a-9b32-205ebb311e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "\n",
    "d0=np.min(atl08['x_atc'])\n",
    "\n",
    "plt.plot(atl08['x_atc']-d0, atl08['h_te_median'], 'o',  markersize=1, color='green', label='h_mean_canopy')\n",
    "plt.plot(atl08['x_atc']-d0, atl08['gedi.value'], 'o',  markersize=1, color='gray', label='gedi elevation')\n",
    "hl=plt.legend(loc=3, frameon=False, markerscale=5)\n",
    "\n",
    "plt.gca().set_ylim([1500, 3500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e9c789-f65a-48ed-ba4b-36ab53a05af5",
   "metadata": {},
   "source": [
    "#### Explanation of what's new\n",
    "\n",
    "* The on-demand ATL08 product (different than the ICESat-2 Standard Data Product) was generated and streamed back to the user.  The ATL08 on-demand product uses University of Texas at Austin's PhoREAL algorithm which was integrated into SlideRule to generate customizable vegetation metrics using ATL03 photon data.\n",
    "\n",
    "* A time range was specified in the request limiting the results to data collected only between the start and stop times supplied.\n",
    "\n",
    "* The `\"atl08_class\"` parameter specified that only photons in ATL03 that were classified as `\"atl08_ground\"`, `\"atl08_canopy\"`, or `\"atl08_top_of_canopy\"` in the ATL08 standard data product are to be supplied to the PhoREAL algorithm and used in the results.\n",
    "\n",
    "* The `\"atl08_fields\"` parameter specifies that the  `\"h_dif_ref\"` variable from the ATL08 standard data product is to be associated with each result returned by SlideRule.  SlideRule attempts to find the value of the variable closest in time to the dynamically generated result.\n",
    "\n",
    "* The `\"phoreal\"` parameter provides the processing parameters for the PhoREAL algorithm.\n",
    "  \n",
    "* The `\"samples\"` parameter provides a list of raster datasets that SlideRule should sample at each generated result.  So for each 100m segment that PhoREAL processes, the server-side code will also sample the `gedil3-elevation` product at the latitude and longitude of that segment and return the value with the results.\n",
    "\n",
    "For a list of raster datasets that are available to sample in SlideRule, see: https://slideruleearth.io/web/rtd/user_guide/GeoRaster.html#asset-directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8369043-c646-4082-a7f3-0f1e8546bc18",
   "metadata": {},
   "source": [
    "### Example 3: Produce GeoParquet of Summer and Winter ICESat-2 Tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6c8d4-b1ef-4ba0-aedf-d73e16085e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sliderule import sliderule, icesat2\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781e4c6-0b01-4359-9b72-fa25e35bae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sliderule.init(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d63f4-06c3-429f-a2d0-bde95055a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sliderule.toregion(\"bathy.geojson\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823c240c-e3db-4682-b9f0-0b44d67f4521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATL03 subsetting request parameters\n",
    "parms = {\n",
    "    \"poly\": region['poly'],\n",
    "    \"srt\": icesat2.SRT_DYNAMIC,\n",
    "    \"len\": 100,\n",
    "    \"res\": 100,\n",
    "    \"pass_invalid\": True,\n",
    "    \"output\": {\n",
    "        \"asset\":\"sliderule-stage\",\n",
    "        \"format\": \"parquet\",\n",
    "        \"as_geo\": True,\n",
    "        \"open_on_complete\": False\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad687fd-9e3e-490e-80e8-aa4e426928e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "atl03_url = icesat2.atl03sp(parms, resources=['ATL03_20230213042035_08341807_006_02.h5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e54ad-530d-4f6a-8c95-4d3602d73994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atl03_url = \"s3://sliderule-public/sliderule.0000000D832589A9.geoparquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d4174-1939-4cb2-9e11-55ac8059adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "atl03 = gpd.pd.read_parquet(atl03_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd2653-7e25-4f58-b484-a021b42c46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "atl03.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea8bd62-a4e9-44c3-8dba-d87ef82a3a1f",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd3ca78-2d1f-478e-a9cf-91b29adb1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca10793-7207-4d49-9a70-20a40d634754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = atl03\n",
    "df = df[df[\"pair\"] == icesat2.LEFT_PAIR]\n",
    "df = df[df[\"track\"] == 3]\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(df['x_atc']+df['segment_dist'], df['height'], 'o',  markersize=1, color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73348d75-d36b-4c00-a055-629d36806e1b",
   "metadata": {},
   "source": [
    "## Part 3: H5Coro - The HDF5 Cloud-Optimized Read-Only Python Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e57036-72cf-43b4-8519-325bc744acfe",
   "metadata": {},
   "source": [
    "`h5coro` is a pure Python implementation of a subset of the HDF5 specification that has been optimized for reading data out of S3. \n",
    "\n",
    "The project has its roots in SlideRule, where a new C++ implementation of the HDF5 specification was developed for performant read access to Earth science datasets stored in AWS S3. Over time, user's of SlideRule began requesting the ability to performantly read HDF5 and NetCDF files out of S3 from their own Python scripts. The result is `h5coro`: the re-implementation in Python of the core HDF5 reading logic that exists in SlideRule. Since then, `h5coro` has become its own project, which will continue to grow and diverge in functionality from its parent implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c2be5-bb57-4aa4-92df-a7330668149a",
   "metadata": {},
   "source": [
    "`h5coro` is optimized for reading HDF5 data in high-latency high-throughput environments. It accomplishes this through a few key design decisions:\n",
    "\n",
    "* __All reads are concurrent.__ Each dataset and/or attribute read by h5coro is performed in its own thread.\n",
    "* __Intelligent range gets__ are used to read as many dataset chunks as possible in each read operation. This drastically reduces the number of HTTP requests to S3 and means there is no longer a need to re-chunk the data (it actually works better on smaller chunk sizes due to the granularity of the request).\n",
    "* __Block caching__ is used to minimize the number of GET requests made to S3. S3 has a large first-byte latency (we've measured it at ~60ms on our systems), which means there is a large penalty for each read operation performed. h5coro performs all reads to S3 as large block reads and then maintains data in a local cache for access to smaller amounts of data within those blocks.\n",
    "* The system is __serverless__ and does not depend on any external services to read the data. This means it scales naturally as the user application scales, and it reduces overall system complexity.\n",
    "* __No metadata repository is needed.__ The structure of the file are cached as they are read so that successive reads to other datasets in the same file will not have to re-read and re-build the directory structure of the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ba463-8a53-4455-a2cd-0604ec18aafa",
   "metadata": {},
   "source": [
    "### For more information:\n",
    "***GitHub***: https://github.com/SlideRuleEarth/h5coro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560afb5e-ae08-4401-a57f-82f5b901da7e",
   "metadata": {},
   "source": [
    "### Example 1: Read ATL03 variables for bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6634f60b-6054-4080-98ac-916b8390be0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Import modules\n",
    "from h5coro import h5coro, s3driver\n",
    "import earthaccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32d25274-5f1d-48ac-aeca-9d20c04d100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Authenticate to Earth Data Login\n",
    "auth = earthaccess.login()\n",
    "s3_creds = auth.get_s3_credentials(daac=\"NSIDC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1eb31a5-94d8-4a9a-8f76-c0c8701059bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (3) Initialize h5coro object\n",
    "granule = \"nsidc-cumulus-prod-protected/ATLAS/ATL03/006/2023/02/13/ATL03_20230213042035_08341807_006_02.h5\"\n",
    "h5obj = h5coro.H5Coro(granule, s3driver.S3Driver, errorChecking=True, verbose=False, credentials=s3_creds, multiProcess=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d235bfa-f19f-4318-93ab-372dbae26b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt3l/heights/h_ph: [-47.941536 -51.9231   -48.09843  -47.873924 -48.12945  -48.118694\n",
      " -48.308052 -48.208042 -47.802708 -48.004234]\n",
      "gt3l/heights/dist_ph_along: [0.7542868  0.76623714 1.4717534  2.187351   2.1880984  2.9048157\n",
      " 2.905563   3.621905   4.337497   5.0545855 ]\n",
      "gt3l/geolocation/segment_dist_x: [17068770.48934802 17068790.54479094 17068810.60023396 17068830.65567708\n",
      " 17068850.7111203  17068870.76656362 17068890.82200704 17068910.87745056\n",
      " 17068930.93289417 17068950.98833789]\n",
      "gt3l/geolocation/segment_ph_cnt: [37 25 44 39 22 40 37 42 35 38]\n"
     ]
    }
   ],
   "source": [
    "# (4) Read the data\n",
    "variables = [\"/gt3l/heights/h_ph\", \"/gt3l/heights/dist_ph_along\", \"/gt3l/geolocation/segment_dist_x\", \"/gt3l/geolocation/segment_ph_cnt\"]\n",
    "promise = h5obj.readDatasets(variables, block=True, enableAttributes=False)\n",
    "for variable in promise:\n",
    "    print(f'{variable}: {promise[variable][0:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a1a215-b0de-45a3-a0c0-d793db26f9d4",
   "metadata": {},
   "source": [
    "#### Explanation of what happened\n",
    "\n",
    "#### (1) Import the necessary packages to use `h5coro`. \n",
    "\n",
    "`h5coro` relies on `earthaccess` for authenticating to Earth Data Login.  The modules a user might want to import are:\n",
    "* `s3driver`: for reading data out of an s3 bucket\n",
    "* `filedriver`: for reading data out of a local file\n",
    "* `webdriver`: for reading data diretly over https (including objects in s3 buckets)\n",
    "* `logger`: for configuring the logging in h5coro\n",
    "\n",
    "#### (2) Authenticate to Earth Data Login\n",
    "\n",
    "In my system I have a `.netrc` file setup with the following line:\n",
    "```\n",
    "machine urs.earthdata.nasa.gov login <my_user_name> password <my_password>\n",
    "```\n",
    "\n",
    "#### (3) Create an h5coro object for the granule that you want to read\n",
    "\n",
    "`h5coro` is object oriented, so all context information associated with the provided granule is stored in the object.  Note that the full path to the granule is needed, including the s3 bucket.\n",
    "\n",
    "#### (4) Read the data\n",
    "\n",
    "`h5coro` implements an asynchronous I/O interface, meaning that when the `readDatasets` function is called, it makes a read \"request\" in the background and returns immediately back to the caller.  The caller receives something called a \"promise\" (or \"future\") which is a promise that data will be there in the future at some point.  You then can do other things while you wait, and when you finally need the data, you have to \"block\" or wait for it to be available.\n",
    "\n",
    "In this example, I set the \"block\" parameter to True so that it would wait right away.  But in more sophisticated examples, other work could have been done by the notebook while waiting for the results of the read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85f44c-29f4-46f5-a720-f171eb54e2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython - SlideRule",
   "language": "python",
   "name": "sliderule_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
